---
permalink: world-state-is-the-wrong-abstraction-for-impact
lw-was-draft-post: "false"
lw-is-af: "true"
lw-is-debate: "false"
lw-page-url: https://www.lesswrong.com/posts/pr3bLc2LtjARfK7nx/world-state-is-the-wrong-abstraction-for-impact
lw-is-question: "false"
lw-posted-at: 2019-10-01T21:03:40.153Z
lw-last-modification: 2020-12-14T23:42:24.235Z
lw-curation-date: None
lw-frontpage-date: 2019-10-01T21:14:03.280Z
lw-was-unlisted: "false"
lw-is-shortform: "false"
lw-num-comments-on-upload: 19
lw-base-score: 67
lw-vote-count: 21
af-base-score: 22
af-num-comments-on-upload: 6
publish: true
title: "World State is the Wrong Abstraction for Impact"
lw-latest-edit: 2020-04-22T21:42:10.102Z
lw-is-linkpost: "false"
tags: 
  - "understanding-the-world"
  - "impact-regularization"
  - "AI"
aliases: 
  - "world-state-is-the-wrong-abstraction-for-impact"
lw-sequence-title: "Reframing Impact"
lw-sequence-image-grid: sequencesgrid/izfzehxanx48hvf10lnl
lw-sequence-image-banner: sequences/zpia9omq0zfhpeyshvev
sequence-link: posts#reframing-impact
prev-post-slug: attainable-utility-theory
prev-post-title: "Attainable Utility Theory: Why Things Matter"
next-post-slug: the-gears-of-impact
next-post-title: "The Gears of Impact"
lw-reward-post-warning: "false"
use-full-width-images: "false"
date_published: 10/01/2019
original_url: https://www.lesswrong.com/posts/pr3bLc2LtjARfK7nx/world-state-is-the-wrong-abstraction-for-impact
skip_import: true
---
![](https://i.imgur.com/DNE5EJg.png)

![](https://i.imgur.com/bimkCyz.png)

![](https://i.imgur.com/uC0LySG.png)

![](https://i.imgur.com/64N3tKB.png )

![](https://i.imgur.com/giAuRyY.png)

![](https://i.imgur.com/Cs2jkZr.png)

![](https://i.imgur.com/mrVMkSH.png)

## ![](https://i.imgur.com/T9MnkcK.png)

![](https://i.imgur.com/dmy8BTO.png)

![](https://i.imgur.com/u0CMsnj.png)

![](https://i.imgur.com/UGMcfsy.png )

![](https://i.imgur.com/Jidk86s.png  )

![](https://i.imgur.com/6Ecn3ug.png)

![](https://i.imgur.com/YmknuEn.png)

![](https://i.imgur.com/1rVGIUj.png)

![](https://i.imgur.com/WjTqF2y.png)

![](https://i.imgur.com/LYNGAta.png)

![](https://i.imgur.com/j0pWEA1.png )

![](https://i.imgur.com/BINRNvJ.png)

![](https://i.imgur.com/C6aYsBj.png )

![](https://i.imgur.com/d9q2zBy.png)

>! These existential crises also muddle our impact algorithm. This isn't what you'd see if impact were primarily about the world state.

![](https://i.imgur.com/WjTqF2y.png)

![](https://i.imgur.com/q09LBrl.png)

![](https://i.imgur.com/zVkGE6q.png  )

![](https://i.imgur.com/GEwXYwT.png ) ![](https://i.imgur.com/CPCaLoM.png)

![](https://i.imgur.com/wLogljp.png )

![](https://i.imgur.com/tiKGyYq.png)

![](https://i.imgur.com/ZsAlmei.png)

## Appendix: We Asked a Wrong Question

How did we go wrong?

> [!quote] [Righting a Wrong Question](https://www.readthesequences.com/Righting-A-Wrong-Question)
>
> When you are faced with an unanswerable question—a question to which it seems impossible to even imagine an answer—there is a simple trick that can turn the question solvable.
>
> Asking “Why do I have free will?” or “Do I have free will?” sends you off thinking about tiny details of the laws of physics, so distant from the macroscopic level that you couldn’t begin to see them with the naked eye. And you’re asking “Why is  $X$ the case?” where $X$ may not be coherent, let alone the case.
>
> “Why do I think I have free will?,” in contrast, is guaranteed answerable. You do, in fact, believe you have free will. This belief seems far more solid and graspable than the ephemerality of free will. And there is, in fact, some nice solid chain of cognitive cause and effect leading up to this belief.


I think what gets you is asking the question "what things are impactful?" instead of "why do I think things are impactful?". Then, you substitute the easier-feeling question of "how different are these world states?". Your fate is sealed; you've anchored yourself on a Wrong Question.

At least, that's what _I_ did.

> [!exercise]
> Someone (me, early last year says that impact is closely related to change in object identities.
> 
> ![](https://i.imgur.com/pnztldk.png)
> 
> Find at least two scenarios which score as low impact by this rule but as high impact by your intuition, or vice versa.
> 
> You have 3 minutes.
> 
> >! Gee, let's see... Losing your keys, the torture of humans on Iniron, being locked in a room, flunking a critical test in college, losing a significant portion of your episodic memory, ingesting a pill which makes you think murder is OK, changing your discounting to be completely myopic, having your heart broken, getting really dizzy, losing your sight.
> >
> >! That's three minutes for me, at least (its length reflects how long I spent coming up with ways I had been wrong).

## Appendix: Avoiding Side Effects

Some plans feel like they have unnecessary _side effects_. Consider "Go to the store" versus "Go to the store and run over a potted plant."

We talk about side effects when they affect our attainable utility (otherwise we don't notice), and they need both a goal ("side") and an ontology (discrete "effects").

Accounting for impact this way misses the point.

Yes, we can think about effects and facilitate academic communication more easily via this frame, but _we should be careful not to guide research from that frame_. This is why I avoided vase examples early on – their prevalence seems like a _symptom of an incorrect frame_.

(Of course, I certainly did my part to make them more prevalent, what with my first post about impact being called _[Worrying about the Vase: Whitelisting](/whitelisting-impact-measure)..._)

#### Notes

- Your ontology can't be _ridiculous_ ("everything is a single state"), but as long as it lets you represent what you care about, it's fine by AU theory.
- Read more about ontological crises at [Rescuing the utility function.](https://arbital.com/p/rescue_utility/)
- Obviously, something has to be physically different for events to feel impactful, but not all differences are impactful. Necessary, but not sufficient.
- AU theory avoids the mind projection fallacy; impact is subjectively objective _because _[probability is subjectively objective](https://www.lesswrong.com/posts/XhaKvQyHzeXdNnFKy/probability-is-subjectively-objective).
- I'm not aware of others explicitly trying to deduce our native algorithm for impact. No one was claiming the ontological theories explain our intuitions, and they didn't have the same "is this a big deal?" question in mind. However, we need to actually understand the problem we're solving, and providing that understanding is one responsibility of an impact measure! Understanding our own intuitions is crucial not just for producing nice equations, but also for getting an intuition for what a "low-impact" Frank would do.