---
permalink: the-gears-of-impact
lw-was-draft-post: None
lw-is-af: "true"
lw-is-debate: "false"
lw-page-url: https://www.lesswrong.com/posts/coQCEe962sjbcCqB9/the-gears-of-impact
lw-linkpost-url: https://www.lesswrong.com/posts/coQCEe962sjbcCqB9/the-gears-of-impact
lw-is-question: "false"
lw-posted-at: 2019-10-07T14:44:51.212Z
lw-last-modification: None
lw-curation-date: None
lw-frontpage-date: 2019-10-07T06:10:45.213Z
lw-was-unlisted: "false"
lw-is-shortform: "false"
lw-num-comments-on-upload: 16
lw-base-score: 54
lw-vote-count: 18
af-base-score: 18
af-num-comments-on-upload: 0
publish: true
title: "The Gears of Impact"
lw-latest-edit: 2019-10-07T15:42:29.068Z
lw-is-linkpost: "false"
tags: 
  - "Impact-Regularization"
  - "Understanding-the-world"
aliases: 
  - "the-gears-of-impact"
lw-sequence-title: Reframing Impact
lw-sequence-image-grid: sequencesgrid/izfzehxanx48hvf10lnl
lw-sequence-image-banner: sequences/zpia9omq0zfhpeyshvev
prev-post-slug: world-state-is-the-wrong-abstraction-for-impact
next-post-slug: seeking-power-is-often-convergently-instrumental-in-mdps
date_published: 10/07/2019
original_url: https://www.lesswrong.com/posts/coQCEe962sjbcCqB9/the-gears-of-impact
---
![](https://i.imgur.com/hKhkvwg.png)

![](https://i.imgur.com/IXogCtA.png)

![](https://i.imgur.com/2r2DVFx.png)

![](https://i.imgur.com/holekcV.png)

![](https://i.imgur.com/SzFSiEc.png)

![](https://i.imgur.com/wCRzqox.png) ![](https://i.imgur.com/BAWF2q1.png)

![](https://i.imgur.com/UCGx4QR.png )

![](https://i.imgur.com/5YOlvLh.png)![](https://i.imgur.com/yA8wkQP.png)![](https://i.imgur.com/QXG2pVK.png)

![](https://i.imgur.com/27F0KkU.png)

![](https://i.imgur.com/B7rMciV.png)

![](https://i.imgur.com/HIfRI7r.png)

![](https://i.imgur.com/ye9suf7.png)

![](https://i.imgur.com/sMgB7yR.png)

![](https://i.imgur.com/lQ1jYfB.png )

![](https://i.imgur.com/b6pDiKi.png)

[​](​![]\(https://i.imgur.com/iRLXEeH.png)![](https://i.imgur.com/iRLXEeH.png)

![](https://i.imgur.com/uRr6YqY.png )

![](https://i.imgur.com/67uR5SE.png)![](https://i.imgur.com/PFqi66W.png)

![](https://i.imgur.com/GBVahyL.png)

![](https://i.imgur.com/SATKmJJ.png)

![](https://i.imgur.com/v338kDc.png)

![](https://i.imgur.com/oqEeta9.png)

![](https://i.imgur.com/epI7152.png)

![](https://i.imgur.com/dvVEmBs.png) [​](​![]\(https://i.imgur.com/HShpS3u.png)![](https://i.imgur.com/HShpS3u.png)

![](https://i.imgur.com/WjTqF2y.png)

![](https://i.imgur.com/dLUrki7.png)

![](https://i.imgur.com/lDbQW2b.jpg )

Scheduling: The remainder of the sequence will be released after some delay.

_Exercise: Why does instrumental convergence happen? Would it be coherent to imagine a reality without it?_

Notes
*   Here, our descriptive theory relies on our ability to have reasonable beliefs about what we'll do, and how things in the world will affect our later decision-making process. No one knows how to formalize that kind of reasoning, so I'm leaving it a black box: we _somehow_ have these reasonable beliefs which are _apparently_ used to calculate AU.
*   In technical terms, AU calculated with the "could" criterion would be closer to an optimal value function, while actual AU seems to be an on-policy prediction, _whatever that means_ in the embedded context. Felt impact corresponds to TD error.
*   This is one major reason I'm disambiguating between AU and EU; in the non-embedded context. In reinforcement learning, AU is a very particular kind of EU:  $V^*(s)$, the expected return under the optimal policy.
*   Framed as a kind of EU, we plausibly use AU to make decisions.
*   I'm not claiming normatively that "embedded agentic" EU _should_ be AU; I'm simply using "embedded agentic" as an adjective.